{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_3_4_Machine_Learning_with_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ie_1Xr5jx01T"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amjadraza/spark-ml-course/blob/main/week-2/Session_3_4_Machine_Learning_with_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yboq60sBPwXq"
      },
      "source": [
        "# Welcome to the course: Machine Learning with Spark (Session 3 & 4)\n",
        "\n",
        "This course is offered by in collaboration of  [AI-LOUNG](http://ai-lounge.com/) & [Datafy2AI](https://www.datafy2ai.com/)\n",
        "\n",
        "Instructor: [Dr. Muhammad Amjad Raza](https://www.linkedin.com/in/amjadraza/)\n",
        "\n",
        "## Refresher:\n",
        "\n",
        "\n",
        "1.   What is Spark?\n",
        "2.   Setting Up Spark within Google Colab Environment\n",
        "3.   Basics of Spark Operations?\n",
        "4.   Spark DataFrame API using PySpark\n",
        "\n",
        "\n",
        "## Learning Objective of this Session:\n",
        "\n",
        "\n",
        "1. Basics of Machine Learning?\n",
        "2. DataPipelines with Spark\n",
        "3. Feature Engineering with Spark\n",
        "4. Feature Engineering with KOALAS\n",
        "4. Machine Leraning with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opFqw5kFQX8V"
      },
      "source": [
        "# A power Point Presentation: An Overview of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwAKNU5Easmb"
      },
      "source": [
        "# A comprehensive Guide: Machine Learning with Spark\n",
        "\n",
        "Machine learning is getting popular in solving real-world problems in almost every business domain. It helps solve the problems using the data which is often unstructured, noisy, and in huge size. With the increase in data sizes and various sources of data, solving formulating and machine learning problems using standard techniques pose a big challenge. Spark is a distributed processing engine using the MapReduce framework to solve problems related to big data and processing of it.\n",
        "\n",
        "Spark framework has its own machine learning module called MLlib. In this session, I will use pyspark and spark MLlib to demonstrate the use of machine learning using distributed processing. Readers will be able to learn the below concept with real examples.\n",
        "\n",
        "1. Setting up Spark in the Google Colaboratory\n",
        "2. Machine Learning Basic Concepts\n",
        "3. Preprocessing and Data Transformation using Spark\n",
        "4. Spark Clustering with pyspark\n",
        "\n",
        "> A working google colab notebook will be provided to reproduce the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEe66bnJyUas"
      },
      "source": [
        "# Setting up Spark 3.1.1 in the Google Colaboratory\n",
        "\n",
        "As a first step, I configure the google colab runtime with spark installation. For details, readers may read my article [Getting Started Spark 3.0.0 in Google Colab](https://medium.com/analytics-vidhya/getting-started-spark3-0-0-with-google-colab-9796d350d78) om medium. \n",
        "\n",
        "We will install below programs\n",
        "\n",
        "* Java 8\n",
        "* spark-3.1.1\n",
        "* Hadoop3.2 \n",
        "* [Findspark](https://github.com/minrk/findspark)\n",
        "\n",
        "you can install the LATEST version of Spark using below set of commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMTS06C70XiN"
      },
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp0T6glIW1Tz"
      },
      "source": [
        "# Install Latest version of Plotly\n",
        "\n",
        "# ! pip uninstall plotly\n",
        "# ! pip install plotly='4.14.3'\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn2xftZMY8hr",
        "outputId": "33b8165a-0060-4062-e9e0-43566d50f1d7"
      },
      "source": [
        "! pip install loguru"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting loguru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okd2AjDJZIiq"
      },
      "source": [
        "from loguru import logger"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DuJKdLldqks"
      },
      "source": [
        "## Environment Variable \n",
        "After installing the spark and Java, set the enviroment variables where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzji3m72doJY"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7M2Wi1jC1c"
      },
      "source": [
        "## Spark Installation test\n",
        "Lets test the installation of spark in our google colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y18KVg34jkXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43ccca4-7eb3-4404-aaee-c9753d5a18f0"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(3, False)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbeR8NEd9aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24eaf039-0b8b-4a43-cf5b-d652f7659032"
      },
      "source": [
        "# make sure the version of pyspark\n",
        "import pyspark\n",
        "logger.info(pyspark.__version__)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-13 04:24:01.724 | INFO     | __main__:<module>:3 - 3.1.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe37dwkcUZ4F"
      },
      "source": [
        "# Machine Learning \n",
        "\n",
        "Once, we have set up the spark in google colab and made sure it is running with the correct version i.e. 3.0.1 in this case, we can start exploring the machine learning API developed on top of Spark. Pyspark is a higher level of API to use spark with python. For this tutorial, I assume the readers have a basic understanding of Machine learning and used SK-Learn for model building and training. Spark MLlib used the same fit and predict structure as in SK-Learn. \n",
        "\n",
        "In order to reproduce the results, I have uploaded the data to my GitHub and can be accessed easily.\n",
        "\n",
        "> Learn by Doing: Use the colab notebook to run it yourself\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsWZqMdvcOhc"
      },
      "source": [
        "# PySpark: MLlib\n",
        "\n",
        "> DataFrame based API is primary API\n",
        "\n",
        "Machine Learning at Scale\n",
        "\n",
        "* **ML Algorithms:** common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
        "* **Featurization:** feature extraction, transformation, dimensionality reduction, and selection\n",
        "* **Pipelines:** tools for constructing, evaluating, and tuning ML Pipelines\n",
        "* **Persistence:** saving and load algorithms, models, and Pipelines\n",
        "* **Utilities:** linear algebra, statistics, data handling, etc.\n",
        "\n",
        "[Reference](https://spark.apache.org/docs/3.1.1/ml-guide.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA9vBqs1dXfr"
      },
      "source": [
        "## Basic Statistics using PySpark\n",
        "\n",
        "When doing Machine Learning Linear Algebra and Basic Stats are key concepts to implement and study.\n",
        "\n",
        "Lets Learn below concepts using PySpark implementation\n",
        "\n",
        "1. Correlation\n",
        "2. Hypothesis testing\n",
        "3. Summarizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MpvVzF7i8uJ"
      },
      "source": [
        "### Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhSdETE7cNSV"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRZ0R8AJkzR2"
      },
      "source": [
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnDNpEKgjCbM",
        "outputId": "cac7583f-f591-4fab-ed10-f711ad3ff3be"
      },
      "source": [
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(SparseVector(4, {0: 1.0, 3: -2.0}),),\n",
              " (DenseVector([4.0, 5.0, 0.0, 3.0]),),\n",
              " (DenseVector([6.0, 7.0, 0.0, 8.0]),),\n",
              " (SparseVector(4, {0: 9.0, 3: 1.0}),)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUoKkzTkjJBP"
      },
      "source": [
        "df = spark.createDataFrame(data, [\"features\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbs7dTtljKUf",
        "outputId": "5b2c23eb-5609-4e1e-fc04-bc0aa75720b9"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(4,[0,3],[1.0,-2.0])|\n",
            "|   [4.0,5.0,0.0,3.0]|\n",
            "|   [6.0,7.0,0.0,8.0]|\n",
            "| (4,[0,3],[9.0,1.0])|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLFeVcyEk-Yt",
        "outputId": "96a68868-3851-4fd0-8432-a1f3ad08ac1c"
      },
      "source": [
        "features_df.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z70Pv4YejU0-",
        "outputId": "0e848036-adc0-4653-ffcc-582b89115da6"
      },
      "source": [
        "r1 = Correlation.corr(features_df, \"features\").head()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.98198051, 1.        ],\n",
            "             [0.98198051, 1.        , 0.98198051],\n",
            "             [1.        , 0.98198051, 1.        ]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbU06n_6lkTA",
        "outputId": "f4c94647-7929-4e77-9d92-05c57aaa9e67"
      },
      "source": [
        "r1[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseMatrix(3, 3, [1.0, 0.982, 1.0, 0.982, 1.0, 0.982, 1.0, 0.982, 1.0], False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzhY9mQ0jHT_",
        "outputId": "a9555277-849b-44db-d6a6-56ca3f969df3"
      },
      "source": [
        "# Rank Correlation\n",
        "r2 = Correlation.corr(features_df, \"features\", \"spearman\").head()\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1., 1., 1.],\n",
            "             [1., 1., 1.],\n",
            "             [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alPSb6nemnlD"
      },
      "source": [
        "### Hypothesis testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3L76Q5AmueV"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "        (0.0, Vectors.dense(1.5, 20.0)),\n",
        "        (1.0, Vectors.dense(1.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 40.0)),\n",
        "        (1.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoGMktw1mwPu",
        "outputId": "1c14dfd8-1595-4095-ba98-a700cad6b6aa"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|  features|\n",
            "+-----+----------+\n",
            "|  0.0|[0.5,10.0]|\n",
            "|  0.0|[1.5,20.0]|\n",
            "|  1.0|[1.5,30.0]|\n",
            "|  0.0|[3.5,30.0]|\n",
            "|  0.0|[3.5,40.0]|\n",
            "|  1.0|[3.5,40.0]|\n",
            "+-----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzRad_4bm45J",
        "outputId": "4a480157-d9e4-4a63-aedf-05117a8106d4"
      },
      "source": [
        "# ChiSquareTest\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOGT8Y_woGho"
      },
      "source": [
        "## ML Pipelines\n",
        "\n",
        "Machine Learning Pipelines: High level APIs built on top DataFrame API \n",
        "\n",
        "* **DataFrame:** This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
        "\n",
        "* **Transformer:** A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
        "\n",
        "* **Estimator:** An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
        "\n",
        "* **Pipeline:** A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
        "\n",
        "* **Parameter:** All Transformers and Estimators now share a common API for specifying parameters.\n",
        "\n",
        "> Train, Predict API format | Inspired by Sk-Learn API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz8zltFmtrq1"
      },
      "source": [
        "## [Advanced] Model Selection/Hyperparameters Tuning\n",
        "\n",
        "* **CrossValidator:** A Class to implement cross validation using k-fold method. Hyperparameters are fit using single fold and validated on other folds.\n",
        "* **TrainValidationSplit:**  A class to fine tune the hyper parameters by spliting the training data into train and valid datasets. \n",
        "* **Evauator:** A metric to measure how well a fitted Model does on held-out test/validation data\n",
        "\n",
        "> Examples: \n",
        "\n",
        "> https://spark.apache.org/docs/3.1.1/ml-tuning.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beb9ISo5p05X"
      },
      "source": [
        "### Example of Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYgganNhpsbO",
        "outputId": "24201609-ec58-4d22-bc9e-3165845d22b7"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# Prepare training documents from a list of (id, text, label) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row  # type: ignore\n",
        "    print(\n",
        "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "            rid, text, str(prob), prediction   # type: ignore\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, spark i j k) --> prob=[0.1596407738787412,0.8403592261212588], prediction=1.000000\n",
            "(5, l m n) --> prob=[0.8378325685476614,0.16216743145233858], prediction=0.000000\n",
            "(6, spark hadoop spark) --> prob=[0.06926633132976266,0.9307336686702373], prediction=1.000000\n",
            "(7, apache hadoop) --> prob=[0.9821575333444208,0.017842466655579203], prediction=0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6LU2ggyojmw"
      },
      "source": [
        "# Data Preparation and Transformations in Spark\n",
        "\n",
        "This section covers the basic steps involved in transformations of input feature data into the format Machine Learning algorithms accept. We will be covering the transformations coming with the SparkML library. To understand or read more about the available spark transformations in 3.1.1, follow the below link.\n",
        "\n",
        "https://spark.apache.org/docs/3.1.1/ml-features.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcSHz1H3ZV7"
      },
      "source": [
        "## Normalize Numeric Data\n",
        "\n",
        "MinMaxScaler is one of the favorite classes shipped with most machine learning libraries. It scaled the data between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxJ9AOFihT4"
      },
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKzJ4FjViv-P"
      },
      "source": [
        "# Create some dummy feature data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCCGoUFGjopG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "96b9fa3e-1e03-4f11-d74c-c59c6c42f563"
      },
      "source": [
        "features_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1|[10.0,10000.0,1.0]|\n",
            "|  2|[20.0,30000.0,2.0]|\n",
            "|  3|[30.0,40000.0,3.0]|\n",
            "+---+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMrx6ok3j5QV"
      },
      "source": [
        "# Apply MinMaxScaler transformation\n",
        "features_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
        "smodel = features_scaler.fit(features_df)\n",
        "sfeatures_df = smodel.transform(features_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94487nHqlB36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "4e9a6d84-25ab-4509-d77e-6501fb301bcb"
      },
      "source": [
        "sfeatures_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|           (3,[],[])|\n",
            "|  2|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
            "|  3|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0fpo-2aoSR6"
      },
      "source": [
        "## Standardize Numeric Data\n",
        "\n",
        "StandardScaler is another well-known class written with machine learning libraries. It normalizes the data between -1 and 1 and converts the data into bell-shaped data. You can demean the data and scale to some variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzgOUyAoeGJ"
      },
      "source": [
        "from pyspark.ml.feature import  StandardScaler\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgD9wMW9lslE"
      },
      "source": [
        "# Create the dummy data\n",
        "features_df = spark.createDataFrame([\n",
        "    (1, Vectors.dense([10.0,10000.0,1.0]),),\n",
        "    (2, Vectors.dense([20.0,30000.0,2.0]),),\n",
        "    (3, Vectors.dense([30.0,40000.0,3.0]),),\n",
        "    \n",
        "],[\"id\", \"features\"] )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAgTOmAkl0uV"
      },
      "source": [
        "# Apply the StandardScaler model\n",
        "features_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd=True, withMean=True)\n",
        "stmodel = features_stand_scaler.fit(features_df)\n",
        "stand_sfeatures_df = stmodel.transform(features_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT1eT52ymYHY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "e0ea4924-a4d4-4617-a5db-455eccba2f93"
      },
      "source": [
        "stand_sfeatures_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+--------------------+\n",
            "| id|          features|           sfeatures|\n",
            "+---+------------------+--------------------+\n",
            "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
            "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
            "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
            "+---+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mcttwMxoIBX"
      },
      "source": [
        "## Bucketize Numeric Data\n",
        "\n",
        "The real data sets come with various ranges and sometimes it is advisable to transform the data into well-defined buckets before plugging into machine learning algorithms.\n",
        "\n",
        "Bucketizer class is handy to transform the data into various buckets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmdLCd_woCDM"
      },
      "source": [
        "from pyspark.ml.feature import  Bucketizer\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSnGOTVPmn9w"
      },
      "source": [
        "# Define the splits for buckets\n",
        "splits = [-float(\"inf\"), -10, 0.0, 10, float(\"inf\")]\n",
        "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
        "b_df = spark.createDataFrame(b_data, [\"features\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3teBrblnT2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "b09615db-ae7e-4d59-8e1e-55e857a2fad6"
      },
      "source": [
        "b_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|features|\n",
            "+--------+\n",
            "|  -800.0|\n",
            "|   -10.5|\n",
            "|    -1.7|\n",
            "|     0.0|\n",
            "|     8.2|\n",
            "|    90.1|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LQA_zZtnWlN"
      },
      "source": [
        "# Transforming data into buckets\n",
        "bucketizer = Bucketizer(splits=splits, inputCol= \"features\", outputCol=\"bfeatures\")\n",
        "bucketed_df = bucketizer.transform(b_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb3bcGkVntXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "8a60bba5-fae4-4e80-d419-c8425dc2cc94"
      },
      "source": [
        "bucketed_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------+\n",
            "|features|bfeatures|\n",
            "+--------+---------+\n",
            "|  -800.0|      0.0|\n",
            "|   -10.5|      0.0|\n",
            "|    -1.7|      1.0|\n",
            "|     0.0|      2.0|\n",
            "|     8.2|      2.0|\n",
            "|    90.1|      3.0|\n",
            "+--------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsnnhLDkorcE"
      },
      "source": [
        "## Tokenize text Data\n",
        "\n",
        "Natural Language Processing is one of the main applications of Machine learning. One of the first steps for NLP is tokenizing the text into words or token. We can utilize the Tokenizer class with SparkML to perform this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tDc75dto1SH"
      },
      "source": [
        "from pyspark.ml.feature import  Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kxgayjio6bq"
      },
      "source": [
        "sentences_df = spark.createDataFrame([\n",
        "    (1, \"This is an introduction to sparkMlib\"),\n",
        "    (2, \"Mlib incluse libraries fro classfication and regression\"),\n",
        "    (3, \"It also incluses support for data piple lines\"),\n",
        "    \n",
        "], [\"id\", \"sentences\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Jm65OppYsK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "cb2166fd-31d7-483b-cc62-5d0591c6cbe5"
      },
      "source": [
        "sentences_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| id|           sentences|\n",
            "+---+--------------------+\n",
            "|  1|This is an introd...|\n",
            "|  2|Mlib incluse libr...|\n",
            "|  3|It also incluses ...|\n",
            "+---+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Jv-WFbpzhz"
      },
      "source": [
        "sent_token = Tokenizer(inputCol = \"sentences\", outputCol = \"words\")\n",
        "sent_tokenized_df = sent_token.transform(sentences_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpVwcI4WqHPu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ad777683-74a4-4cf1-b3e2-9d99db7221a7"
      },
      "source": [
        "sent_tokenized_df.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib']),\n",
              " Row(id=2, sentences='Mlib incluse libraries fro classfication and regression', words=['mlib', 'incluse', 'libraries', 'fro', 'classfication', 'and', 'regression']),\n",
              " Row(id=3, sentences='It also incluses support for data piple lines', words=['it', 'also', 'incluses', 'support', 'for', 'data', 'piple', 'lines'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF37QTFoqby6"
      },
      "source": [
        "##TF-IDF\n",
        "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Using the above-tokenized data, Let us apply the TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kproZrg-qfLt"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZir1QWWqyri"
      },
      "source": [
        "hashingTF = HashingTF(inputCol = \"words\", outputCol = \"rawfeatures\", numFeatures = 20)\n",
        "sent_fhTF_df = hashingTF.transform(sent_tokenized_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzzwucNQrVEY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7faa4689-2de8-404e-9653-33e58ac57a3e"
      },
      "source": [
        "sent_fhTF_df.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WWlxVe8rezv"
      },
      "source": [
        "idf = IDF(inputCol = \"rawfeatures\", outputCol = \"idffeatures\")\n",
        "idfModel = idf.fit(sent_fhTF_df)\n",
        "tfidf_df = idfModel.transform(sent_fhTF_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldrtqjSTr8ZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "47b847e9-0eec-4629-c9ba-b6674529bb3e"
      },
      "source": [
        "tfidf_df.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=1, sentences='This is an introduction to sparkMlib', words=['this', 'is', 'an', 'introduction', 'to', 'sparkmlib'], rawfeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0}), idffeatures=SparseVector(20, {6: 0.5754, 8: 0.6931, 9: 0.0, 10: 0.6931, 13: 0.2877}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nILjvBXDhszw"
      },
      "source": [
        "> User can play with various transformations depending on the requirements of the problem in-hand. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNo8k78Os2si"
      },
      "source": [
        "# Clustering Using PySpark\n",
        "\n",
        "Clustering is a machine learning technique where the data is grouped into a reasonable number of classes using the input features. In this section, we study the basic application of clustering techniques using the spark ML framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFkM7hQs-5T"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qvl20sVjlsg"
      },
      "source": [
        "# Downloading the clustering dataset\n",
        "!wget -q 'https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/clustering_dataset.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPdJC2bkJNH"
      },
      "source": [
        "Load the clustering data stored in csv format using spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTXI1_UPtUhp"
      },
      "source": [
        "# Read the data.\n",
        "clustering_file_name ='clustering_dataset.csv'\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(clustering_file_name)\n",
        "cluster_df = spark.read.csv(clustering_file_name, header=True,inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHhNCsYkSST"
      },
      "source": [
        "Convert the tabular data into vectorized format using `VectorAssembler` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7J03PqWtUsF"
      },
      "source": [
        "# Coverting the input data into features column\n",
        "vectorAssembler = VectorAssembler(inputCols = ['col1', 'col2', 'col3'], outputCol = \"features\")\n",
        "vcluster_df = vectorAssembler.transform(cluster_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fhb3cHvvlcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "36c9b71b-91c3-48be-9475-1e05bb91ade5"
      },
      "source": [
        "vcluster_df.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+----+--------------+\n",
            "|col1|col2|col3|      features|\n",
            "+----+----+----+--------------+\n",
            "|   7|   4|   1| [7.0,4.0,1.0]|\n",
            "|   7|   7|   9| [7.0,7.0,9.0]|\n",
            "|   7|   9|   6| [7.0,9.0,6.0]|\n",
            "|   1|   6|   5| [1.0,6.0,5.0]|\n",
            "|   6|   7|   7| [6.0,7.0,7.0]|\n",
            "|   7|   9|   4| [7.0,9.0,4.0]|\n",
            "|   7|  10|   6|[7.0,10.0,6.0]|\n",
            "|   7|   8|   2| [7.0,8.0,2.0]|\n",
            "|   8|   3|   8| [8.0,3.0,8.0]|\n",
            "|   4|  10|   5|[4.0,10.0,5.0]|\n",
            "+----+----+----+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVv_Z4mukgrW"
      },
      "source": [
        "Once the data is prepared into the format MLlib can use for models, now we can define and train the clustering algorithm such as K-Means. We can define the number of clusters and initialize the seed as done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIpipYg1tUve"
      },
      "source": [
        "# Applying the k-means algorithm\n",
        "kmeans = KMeans().setK(3)\n",
        "kmeans = kmeans.setSeed(1)\n",
        "kmodel = kmeans.fit(vcluster_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Ny5xyzk_rI"
      },
      "source": [
        "After training has been finished, let us print the centers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut2LEbdXuveX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5560d7c0-4c93-4462-c76f-c9e71dff445d"
      },
      "source": [
        "centers = kmodel.clusterCenters()\n",
        "print(\"The location of centers: {}\".format(centers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The location of centers: [array([35.88461538, 31.46153846, 34.42307692]), array([80.        , 79.20833333, 78.29166667]), array([5.12, 5.84, 4.84])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7pFs60plIyk"
      },
      "source": [
        "There are various kind of clustering algorithms implemented in MLlib. Bisecting K-Means Clustering is another popular method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hUw_1kLuvmk"
      },
      "source": [
        "# Applying Hierarchical Clustering\n",
        "bkmeans = BisectingKMeans().setK(3)\n",
        "bkmeans = bkmeans.setSeed(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSZ3thLXuvpB"
      },
      "source": [
        "bkmodel = bkmeans.fit(vcluster_df)\n",
        "bkcneters = bkmodel.clusterCenters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3mkhWUAxP3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "87021e4f-804b-4900-c8df-6dbd34c13839"
      },
      "source": [
        "bkcneters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([5.12, 5.84, 4.84]),\n",
              " array([35.88461538, 31.46153846, 34.42307692]),\n",
              " array([80.        , 79.20833333, 78.29166667])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skDZs6YBlu8M"
      },
      "source": [
        "To read more about the clustering methods implemented in MLlib, follow below link. https://spark.apache.org/docs/3.1.1/ml-clustering.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie_1Xr5jx01T"
      },
      "source": [
        "# Classification Using PySpark\n",
        "\n",
        "Classification is one of the widely used Machine algorithms and almost every data engineer and data scientist must know about these algorithms. Once the data is loaded and prepared, I will demonstrate three classification algorithms.\n",
        "\n",
        "1. NaiveBayes Classification\n",
        "2. Multi-Layer Perceptron Classification\n",
        "3. Decision Trees Classification\n",
        "\n",
        "\n",
        "We explore the supervised classification algorithms using [IRIS data]( https://archive.ics.uci.edu/ml/datasets/iris). I have uploaded the data into my GitHub to reproduce the results. Users can download the data using below command.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO1pGD_emlX6"
      },
      "source": [
        "# Downloading the clustering data\n",
        "!wget -q \"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1NXDXsAowZ0"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpirMOBhrk8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "97f3295a-d0d4-4fb5-d6bf-d83c5cd84b05"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3            4\n",
              "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
              "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
              "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
              "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
              "4  5.0  3.6  1.4  0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-CpGJAcrGte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9d7bca3-a193-4454-e960-d802f1752457"
      },
      "source": [
        "spark.createDataFrame(df, columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[c_0: double, c_1: double, c_2: double, c_3: double, c4 : string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foVc6Tm-3dYk"
      },
      "source": [
        "## Preprocessing the Iris Data\n",
        "\n",
        "In this section, we will be using the IRIS data to understand the classification. To perform ML models, we apply the preprocessing step on our input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grXJWKRwxQBT"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import  StringIndexer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5jGQaU8xP6A"
      },
      "source": [
        "# Read the iris data\n",
        "df_iris = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/iris.csv\", header=None)\n",
        "iris_df = spark.createDataFrame(df_iris)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBuUFzZurZv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "66fe2cdf-da20-452f-bb51-4307fdde5724"
      },
      "source": [
        "iris_df.show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk2z99ic0Mg6"
      },
      "source": [
        "# Rename the columns\n",
        "iris_df = iris_df.select(col(\"0\").alias(\"sepal_length\"),\n",
        "                         col(\"1\").alias(\"sepal_width\"),\n",
        "                         col(\"2\").alias(\"petal_length\"),\n",
        "                         col(\"3\").alias(\"petal_width\"),\n",
        "                         col(\"4\").alias(\"species\"),\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN6zt5g0Mos"
      },
      "source": [
        "# Converting the columns into features\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "                                  outputCol = \"features\")\n",
        "viris_df = vectorAssembler.transform(iris_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgmRVT1M0MzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "003de15b-10dd-4735-e45f-736a753a11c2"
      },
      "source": [
        "viris_df.show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
            "|4.7         |3.2        |1.3         |0.2        |Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
            "|4.6         |3.1        |1.5         |0.2        |Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
            "|5.0         |3.6        |1.4         |0.2        |Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cguoNqa30M4k"
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"species\", outputCol = \"label\")\n",
        "iviris_df = indexer.fit(viris_df).transform(viris_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo72QJfT27c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "40552d7f-1263-4bd6-efa0-c8569b1bbf60"
      },
      "source": [
        "iviris_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "|5.1         |3.5        |1.4         |0.2        |Iris-setosa|[5.1,3.5,1.4,0.2]|0.0  |\n",
            "|4.9         |3.0        |1.4         |0.2        |Iris-setosa|[4.9,3.0,1.4,0.2]|0.0  |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWwghrd3oT1"
      },
      "source": [
        "## Naive Bayes Calssification\n",
        "\n",
        "Once the data is prepared, we are ready to apply the first classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tou1Tk0o27ky"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of8-W2PJ27nQ"
      },
      "source": [
        "# Create the traing and test splits\n",
        "splits = iviris_df.randomSplit([0.6,0.4], 1)\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdrN8lU4fM2"
      },
      "source": [
        "# Apply the Naive bayes classifier\n",
        "nb = NaiveBayes(modelType=\"multinomial\")\n",
        "nbmodel = nb.fit(train_df)\n",
        "predictions_df = nbmodel.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TccH0PnU4fTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "444b03ec-f5d2-4d9c-9a6a-c0d47d1f7e07"
      },
      "source": [
        "predictions_df.show(1, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species    |features         |label|rawPrediction                                               |probability                                                 |prediction|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "|4.3         |3.0        |1.1         |0.1        |Iris-setosa|[4.3,3.0,1.1,0.1]|0.0  |[-9.966434726497221,-11.294595492758821,-11.956012812323921]|[0.7134106367667451,0.18902823898426235,0.09756112424899269]|0.0       |\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+-----+------------------------------------------------------------+------------------------------------------------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roOzUt5q5F45"
      },
      "source": [
        "Let us Evaluate the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QO9iPYl5Dge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38eb6d2d-e627-41d6-816a-85d4da4bc2ea"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nbaccuracy = evaluator.evaluate(predictions_df)\n",
        "nbaccuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8275862068965517"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxAru6v05jY4"
      },
      "source": [
        "## Multilayer Perceptron Classification\n",
        "\n",
        "The second classifier we will be investigating is a Multi-layer perceptron. In this tutorial, I am not going into details of the optimal MLP network for this problem however in practice, you research the optimal network suitable to the problem in hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrFeWep35qJC"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thL3QCLF5qMJ"
      },
      "source": [
        "# Define the MLP Classifier\n",
        "layers = [4,5,5,3]\n",
        "mlp = MultilayerPerceptronClassifier(layers = layers, seed=1)\n",
        "mlp_model = mlp.fit(train_df)\n",
        "mlp_predictions = mlp_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqXIlpQ5qYP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b592ab41-5a3f-4a56-9eb4-ecabfceadb00"
      },
      "source": [
        "# Evaluate the MLP classifier\n",
        "mlp_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
        "mlp_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwfdn_e7CuL"
      },
      "source": [
        "## Decision Trees Classification\n",
        "\n",
        "Another common classifier in the ML family is the Decision Tree Classifier, in this section, we explore this classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0iWDg9v5qmq"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTx4I55h5qpf"
      },
      "source": [
        "# Define the DT Classifier \n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7gN5hXn5qsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fcf33e3-3f2b-4687-a4ab-4c062c85e61a"
      },
      "source": [
        "# Evaluate the DT Classifier\n",
        "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
        "dt_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9827586206896551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "derIzH6ovgTW"
      },
      "source": [
        "Apart from the above three demonstrated classification algorithms, Spark MLlib has also many other implementations of classification algorithms. Details of the implemented classification algorithms can be found at below link.\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification\n",
        "\n",
        "It is highly recommended to try some of the classification algorithms to get hands-on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIr1nhag7-BC"
      },
      "source": [
        "# Regression using PySpark\n",
        "\n",
        "In this section, we explore the Machine learning models for regression problems using pyspark.Regression models are helpful in predicting the future values using the past data. \n",
        "\n",
        "We will use the [Combined Cycle Power Plant](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant) data set to predict the net hourly electrical output (EP). I have uploaded the data to my GitHub so that users can reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8KB4WCu5qWD"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBckB0VJu7q_"
      },
      "source": [
        "# Read the iris data\n",
        "df_ccpp = pd.read_csv(\"https://raw.githubusercontent.com/amjadraza/blogs-data/master/spark_ml/ccpp.csv\")\n",
        "pp_df = spark.createDataFrame(df_ccpp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK_cSfju5qOz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "68c98013-ddde-427a-b618-62b327e54b06"
      },
      "source": [
        "pp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-------+-----+------+\n",
            "|AT   |V    |AP     |RH   |PE    |\n",
            "+-----+-----+-------+-----+------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|\n",
            "|25.18|62.96|1020.04|59.08|444.37|\n",
            "+-----+-----+-------+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpwadf1-zpI"
      },
      "source": [
        "# Create the feature column using VectorAssembler class\n",
        "vectorAssembler = VectorAssembler(inputCols =[\"AT\", \"V\", \"AP\", \"RH\"], outputCol = \"features\")\n",
        "vpp_df = vectorAssembler.transform(pp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPwTYHjl-zy8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "cd85264a-8fce-4ce0-9384-3944092461fa"
      },
      "source": [
        "vpp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhLBqPGUZ3jH"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "We start with simplest regression technique i.e. Linear Regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scO6fex0-z62"
      },
      "source": [
        "# Define and fit Linear Regression\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"PE\")\n",
        "lr_model = lr.fit(vpp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQxcXMeB-0BQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "65dc3ff5-0bda-4333-dea6-9cd0528f6aa1"
      },
      "source": [
        "# Print and save the Model output\n",
        "lr_model.coefficients\n",
        "lr_model.intercept\n",
        "lr_model.summary.rootMeanSquaredError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.557126016749486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbAUizWd-zsC"
      },
      "source": [
        "#lr_model.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6TgFsHaAF6-"
      },
      "source": [
        "## Decision Tree Regression\n",
        "\n",
        "In thsi section, we explore the Decision Tree Regression commonly used in Machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhYf7s5p-zmR"
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t77YAs-yV-GY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "e8942ab2-ba62-4701-b2b9-885b0791dfae"
      },
      "source": [
        "vpp_df.show(2, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|AT   |V    |AP     |RH   |PE    |features                   |\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
            "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
            "+-----+-----+-------+-----+------+---------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHK17ZfW5QF"
      },
      "source": [
        "# Define train and test data split\n",
        "splits = vpp_df.randomSplit([0.7,0.3])\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrlTie1sV-Jr"
      },
      "source": [
        "# Define the Decision Tree Model \n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "dt_model = dt.fit(train_df)\n",
        "dt_predictions = dt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-wJEvSZaC3Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "2839d5c5-6e33-41b1-c978-4747defb79f8"
      },
      "source": [
        "dt_predictions.show(1, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-------+-----+------+--------------------------+-----------------+\n",
            "|AT  |V    |AP     |RH   |PE    |features                  |prediction       |\n",
            "+----+-----+-------+-----+------+--------------------------+-----------------+\n",
            "|3.31|39.42|1024.05|84.31|487.19|[3.31,39.42,1024.05,84.31]|486.1117703349283|\n",
            "+----+-----+-------+-----+------+--------------------------+-----------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nRcmpivV-pu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac8eabc8-b1e0-4ac7-a8c1-3ad259f5fe50"
      },
      "source": [
        "# Evaluate the Model\n",
        "dt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "dt_rmse = dt_evaluator.evaluate(dt_predictions)\n",
        "print(\"The RMSE of Decision Tree regression Model is {}\".format(dt_rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The RMSE of Decision Tree regression Model is 4.451790078736588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpbIb8DSaeSq"
      },
      "source": [
        "## Gradient Boosting Decision Tree Regression\n",
        "\n",
        "Gradient Boosting is another common choice among ML professionals. Let us try the GBM in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAv99jLyV-e5"
      },
      "source": [
        "from pyspark.ml.regression import GBTRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njma6SqaV-Ye"
      },
      "source": [
        "# Define the GBT Model\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDDGMtZtV-V0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b64d5173-1da6-426c-b0d5-c7f6f6a741d6"
      },
      "source": [
        "# Evaluate the GBT Model\n",
        "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
        "print(\"The RMSE of GBT Tree regression Model is {}\".format(gbt_rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The RMSE of GBT Tree regression Model is 4.035802933864555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM71799KwJ3I"
      },
      "source": [
        "Apart from the above-demonstrated regression algorithms, Spark MLlib has also many other implementations of regression algorithms. Details of the implemented regression algorithms can be found at the below link.\n",
        "\n",
        "https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#regression\n",
        "\n",
        "It is highly recommended to try some of the regression algorithms to get hands-on and play with the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjUld8S6bRHz"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "In this Session, I have tried to give the readers an opportunity to learn and implement basic Machine Learning algorithms using PySpark. Spark not only provide the benefit of distributed processing but also can handle a large amount of data to be processing. To summarise, we have covered below topics/algorithms\n",
        "\n",
        "* Setting up the spark 3.1.1 in Google Colab\n",
        "* Overview of PySpark's MLlib Module\n",
        "* Overview of Data Transformations using PySpark\n",
        "* Clustering algorithms using PySpark\n",
        "* Classification problems using PySpark\n",
        "* Regression Problems using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKdMGKME8qLi"
      },
      "source": [
        "# References Readings/Links\n",
        "\n",
        "1. https://spark.apache.org/docs/latest/ml-features.html\n",
        "2. https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#regression\n",
        "3. https://spark.apache.org/docs/3.0.1/ml-clustering.html\n",
        "4. https://spark.apache.org/docs/3.0.1/ml-classification-regression.html#classification\n"
      ]
    }
  ]
}